{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from translate.storage.tmx import tmxfile\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "rawdata_path = '/Users/alexlo/Desktop/Project/Chai_Trans/rawdata/trados_tmx/'\n",
    "workdata_path = '/Users/alexlo/Desktop/Project/Chai_Trans/workdata/'\n",
    "logdata_path = '/Users/alexlo/Desktop/Project/Chai_Trans/logdata'\n",
    "os.chdir(rawdata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmx_to_df(path: str, from_: str, to_:str) -> pd.DataFrame:\n",
    "    with open(path, 'rb') as fin:\n",
    "        tmx_file = tmxfile(fin, from_, to_)\n",
    "    data = []\n",
    "    for node in tmx_file.unit_iter():\n",
    "        data.append([node.source, node.target])\n",
    "    df = pd.DataFrame(data, columns=[from_, to_])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'CH(Simplified)-EN.tmx'\n",
    "file2 = 'CH(Simplified)-EN_who.tmx'\n",
    "df1 = tmx_to_df(file1, 'zh', 'en')\n",
    "df2 = tmx_to_df(file2, 'zh', 'en')\n",
    "df = pd.concat([df1, df2], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48894, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ç„¦è·èª¿ç¯€æ©Ÿæ§‹</td>\n",
       "      <td>FOCAL LENGTH ADJUSTMENT MECHANISM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ä¸€ç¨®ç„¦è·èª¿ç¯€æ©Ÿæ§‹ï¼Œé©æ–¼èª¿ç¯€ä½æ–¼ä¸€æŠ•å½±æ©Ÿçš„ä¸€æ®¼é«”å…§çš„ä¸€ç„¦è·èª¿ç¯€ä»¶ã€‚</td>\n",
       "      <td>A focal length adjustment mechanism, adapted f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ç„¦è·èª¿ç¯€æ©Ÿæ§‹åŒ…æ‹¬ä¸€æ—‹éˆ•åŠä¸€ç¬¬ä¸€è¡Œç¨‹èª¿æ•´ä»¶ã€‚</td>\n",
       "      <td>The focal length adjustment mechanism includes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>æ—‹éˆ•å±€éƒ¨åœ°å¤–éœ²æ–¼æ®¼é«”ã€‚</td>\n",
       "      <td>The knob is partially exposed to the housing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>æ—‹éˆ•è½‰å‹•ï¼Œå¸¶å‹•ç¬¬ä¸€è¡Œç¨‹èª¿æ•´ä»¶ä»¥ç¬¬ä¸€æ¨è»¸ç‚ºä¸­å¿ƒè½‰å‹•ã€‚</td>\n",
       "      <td>The knob rotates to drive the first stroke adj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 zh  \\\n",
       "0                            ç„¦è·èª¿ç¯€æ©Ÿæ§‹   \n",
       "1  ä¸€ç¨®ç„¦è·èª¿ç¯€æ©Ÿæ§‹ï¼Œé©æ–¼èª¿ç¯€ä½æ–¼ä¸€æŠ•å½±æ©Ÿçš„ä¸€æ®¼é«”å…§çš„ä¸€ç„¦è·èª¿ç¯€ä»¶ã€‚   \n",
       "2             ç„¦è·èª¿ç¯€æ©Ÿæ§‹åŒ…æ‹¬ä¸€æ—‹éˆ•åŠä¸€ç¬¬ä¸€è¡Œç¨‹èª¿æ•´ä»¶ã€‚   \n",
       "3                       æ—‹éˆ•å±€éƒ¨åœ°å¤–éœ²æ–¼æ®¼é«”ã€‚   \n",
       "4         æ—‹éˆ•è½‰å‹•ï¼Œå¸¶å‹•ç¬¬ä¸€è¡Œç¨‹èª¿æ•´ä»¶ä»¥ç¬¬ä¸€æ¨è»¸ç‚ºä¸­å¿ƒè½‰å‹•ã€‚   \n",
       "\n",
       "                                                  en  \n",
       "0                  FOCAL LENGTH ADJUSTMENT MECHANISM  \n",
       "1  A focal length adjustment mechanism, adapted f...  \n",
       "2  The focal length adjustment mechanism includes...  \n",
       "3      The knob is partially exposed to the housing.  \n",
       "4  The knob rotates to drive the first stroke adj...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(workdata_path)\n",
    "df.to_json('tmx_zh_en.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset in Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 24447 examples [00:00, 215962.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "os.chdir(workdata_path)\n",
    "raw_datasets = load_dataset('json', data_files='tmx_zh_en.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©¦è‘—åªç”¨100è¡Œä¾†è¨“ç·´\n",
    "raw_datasets['train'] = raw_datasets['train'].select(range(100, 200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zh': ['æ­¤å¤–ï¼Œåœ¨æœ¬å¯¦æ–½ä¾‹ä¸­ï¼Œé˜»å°¼çµ„ä»¶140é‚„è¨­ç½®æ–¼ç¬¬äºŒè¡Œç¨‹èª¿æ•´ä»¶130ã€‚',\n",
       "  'å…·é«”åœ°èªªï¼Œé˜»å°¼çµ„ä»¶140çš„æŠµå£“ä»¶142(å·¦æ–¹çš„æŠµå£“ä»¶142)è¨­ç½®æ–¼ç¬¬äºŒæ¨è»¸135ä¸Šä»¥å°ç¬¬äºŒæ¨è»¸135å‘ä¸‹æ–½å£“ã€‚',\n",
       "  'å¦‚æ­¤ï¼Œç¬¬ä¸€ç«¯éƒ¨121çš„ä½ç½®ä¾¿èƒ½å¤ è¢«ä¿æŒã€‚',\n",
       "  'å› æ­¤ï¼Œç¬¬ä¸€è¡Œç¨‹èª¿æ•´ä»¶120åœ¨ç¬¬ä¸€æ¨è»¸127è™•ä¾¿æœƒå—åˆ°æŠµå£“ä»¶142çš„ä¸‹å£“åŠ›æ‰€å½¢æˆçš„æ‘©æ“¦åŠ›ï¼Œè€Œä¸æœƒéš¨æ„è½‰å‹•ã€‚',\n",
       "  'å› æ­¤ï¼Œç¬¬äºŒè¡Œç¨‹èª¿æ•´ä»¶130åœ¨ç¬¬äºŒæ¨è»¸135è™•ä¾¿æœƒå—åˆ°æŠµå£“ä»¶142çš„ä¸‹å£“åŠ›æ‰€å½¢æˆçš„æ‘©æ“¦åŠ›è€Œä¸æœƒéš¨æ„è½‰å‹•ï¼Œç¬¬å››ç«¯éƒ¨133çš„ä½ç½®ä¾¿èƒ½å¤ è¢«ä¿æŒã€‚',\n",
       "  'å› æ­¤ï¼Œæ—‹éˆ•110å¯é€£å¸¶åœ°ä¿æŒä½ç½®ï¼Œå¾…ä½¿ç”¨è€…æ–½åŠ›è½‰å‹•æ—‹éˆ•110ï¼Œå…‹æœä¸Šè¿°æ‘©æ“¦åŠ›ï¼Œè€Œå¯ä½¿ç¬¬äºŒè¡Œç¨‹èª¿æ•´ä»¶130èˆ‡ç¬¬ä¸€è¡Œç¨‹èª¿æ•´ä»¶120è½‰å‹•ã€‚',\n",
       "  'ç”±åœ–1å¯è¦‹ï¼Œé˜»å°¼çµ„ä»¶140é‚„å¯é¸æ“‡åœ°åŒ…æ‹¬ä¸€ç·©è¡ä»¶144ã€‚',\n",
       "  'ç·©è¡ä»¶144è¨­ç½®æ–¼æŠµå£“ä»¶142èˆ‡ç¬¬ä¸€æ¨è»¸127ä¹‹é–“ï¼Œç·©è¡ä»¶144å—æŠµå£“ä»¶142æ“ å£“è®Šå½¢ã€‚',\n",
       "  'é€™æ¨£çš„è¨­è¨ˆå¯ä½¿å¾—æŠµå£“ä»¶142é€éç·©è¡ä»¶144ä¾†èª¿æ•´ä¸‹å£“åŠ›ï¼Œä»¥å…ç¬¬ä¸€æ¨è»¸127è™•çš„æ‘©æ“¦åŠ›éå¤§ã€‚',\n",
       "  'ç•¶ç„¶ï¼Œçµ„è£è€…ä¹Ÿå¯ä»¥é€éèª¿æ•´æŠµå£“ä»¶142åœ¨æ²¿è‘—è»¸å‘Açš„èºæ¥ä½ç½®ï¼Œä¾†æä¾›ä¸åŒçš„ä¸‹å£“åŠ›ã€‚'],\n",
       " 'en': ['In addition, in this embodiment, the 140 is also set on the 130.',\n",
       "  'Specifically, the 142 of the 140 (the 142 on the left) is set on the 135 to puts downward pressure on the 135.',\n",
       "  'As a result, the position of the 121 can be remained.',\n",
       "  'Therefore, the 120 on the 127 is subjected to the friction formed by the downward pressure of the 142, and will not rotate freely.',\n",
       "  'Therefore, the 130 on the 135 is subjected to the friction formed by the downward pressure of the 142, and will not rotate freely, the position of the 133 can be remained.',\n",
       "  'As a result, the position of the 110 is also remained, and when the user overcomes the friction described above to rotate the 110, the 130 and the 120 is rotated.',\n",
       "  'As shown in FIG. 1, the 140 may also include a cushioning member 144.',\n",
       "  'The cushioning member 144 is set between the pressing member 142 and the first pivot 127, the cushioning member 144 is deformed by the pressure of the pressing member 142.',\n",
       "  'Such a design enables the 142 to adjust the downward pressure through the 144, so as to prevent the friction on the 127 being excessive.',\n",
       "  'Certainly, assemblers can also provide different downward pressure through adjusting the screwing position along the axis A of the 142.']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['zh', 'en'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['zh', 'en'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zh': 'åœ¨ä¸€å¯¦æ–½ä¾‹ä¸­ï¼Œç£å¸ä»¶146ã€147ä¹Ÿå¯ä»¥æ˜¯å…¶ä¸­ä¸€è€…æ˜¯ç£éµï¼Œå¦ä¸€è€…æ˜¯å¯è¢«ç£éµå¸å¼•çš„é‡‘å±¬ã€‚',\n",
       " 'en': 'In an embodiment, one of the magnetic attraction members 146 and 147 can also be a magnet, while the other one is a metal that can be attracted by the magnet.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'The second line structure is suitable to connect to another group of connectors.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"charliealex123/marian-finetuned-kde4-zh-to-en\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"ç¬¬äºŒçµ„ç·šè·¯çµæ§‹é©åˆé€£æ¥å¦ä¸€çµ„æ¥åˆå™¨ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In an embodiment, one of the magnetic attraction members 146 and 147 can also be a magnet, while the other one is a metal that can be attracted by the magnet.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1][\"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "zh_sentence = split_datasets[\"train\"][1][\"zh\"]\n",
    "en_sentence = split_datasets[\"train\"][1][\"en\"]\n",
    "\n",
    "inputs = tokenizer(zh_sentence)\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(en_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"zh\"]]\n",
    "    targets = [ex for ex in examples[\"en\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   66,    57, 61273,     2,   139,     4,     3, 49328, 49546,   298,\n",
       "         33709,     6, 33154,   122,    79,    32,    12, 51279,     2,   599,\n",
       "             3,    85,   139,    30,    12, 21202,    19,   122,    32, 27457,\n",
       "            29,     3, 51279,     5,     0,  -100,  -100],\n",
       "        [ 5063,     2,     3, 21581,    18,     3, 31103,    30,  6295,     8,\n",
       "             3, 55047,  7327,    29,     3, 32220,  5988,     4,     3, 34626,\n",
       "             2,     6,    73,    54, 48118,  9335,     2,     3,  1445,     4,\n",
       "             3, 30294,   122,    32,  2912,     5,     0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 57, 61273, 2, 139, 4, 3, 49328, 49546, 298, 33709, 6, 33154, 122, 79, 32, 12, 51279, 2, 599, 3, 85, 139, 30, 12, 21202, 19, 122, 32, 27457, 29, 3, 51279, 5, 0]\n",
      "[5063, 2, 3, 21581, 18, 3, 31103, 30, 6295, 8, 3, 55047, 7327, 29, 3, 32220, 5988, 4, 3, 34626, 2, 6, 73, 54, 48118, 9335, 2, 3, 1445, 4, 3, 30294, 122, 32, 2912, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/1v6fw56935j1czts11tm_7r00000gq/T/ipykernel_42546/200089639.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/load.py:752: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 46.750469682990165,\n",
       " 'counts': [11, 6, 4, 3],\n",
       " 'totals': [12, 11, 10, 9],\n",
       " 'precisions': [91.66666666666667,\n",
       "  54.54545454545455,\n",
       "  40.0,\n",
       "  33.333333333333336],\n",
       " 'bp': 0.9200444146293233,\n",
       " 'sys_len': 12,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1.683602693167689,\n",
       " 'counts': [1, 0, 0, 0],\n",
       " 'totals': [4, 3, 2, 1],\n",
       " 'precisions': [25.0, 16.666666666666668, 12.5, 12.5],\n",
       " 'bp': 0.10539922456186433,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This This This This\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-zh-to-en\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Logging\n",
    "1. 100 å¥è©±å‰å¾Œè©•ä¼°éƒ½å·®ä¸å¤š 15s\n",
    "2. è¨“ç·´è¦ 9m14s(6minè·‘å¾ªç’°æœ€å¾Œ3minä¸çŸ¥é“è¡å•¥å°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.314296007156372,\n",
       " 'eval_bleu': 28.452433575353787,\n",
       " 'eval_runtime': 14.5938,\n",
       " 'eval_samples_per_second': 0.685,\n",
       " 'eval_steps_per_second': 0.069}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [01:54<03:26, 51.52s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [04:04<01:50, 55.27s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [05:52<00:00, 51.73s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [05:56<00:00, 59.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 356.7753, 'train_samples_per_second': 0.757, 'train_steps_per_second': 0.017, 'train_loss': 1.84809414545695, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=1.84809414545695, metrics={'train_runtime': 356.7753, 'train_samples_per_second': 0.757, 'train_steps_per_second': 0.017, 'train_loss': 1.84809414545695, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.182519555091858,\n",
       " 'eval_bleu': 46.86788757848948,\n",
       " 'eval_runtime': 17.5937,\n",
       " 'eval_samples_per_second': 0.568,\n",
       " 'eval_steps_per_second': 0.057,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310M/310M [04:16<00:00, 1.21MB/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/charliealex123/marian-finetuned-kde4-zh-to-en/commit/b395ea3d3c31553cc64dd4d210c8a41f2ce6bbdc', commit_message='Training complete', commit_description='', oid='b395ea3d3c31553cc64dd4d210c8a41f2ce6bbdc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å®Œæ•´å¾ªç’°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'charliealex123/marian-finetuned-kde4-zh-to-en'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"marian-finetuned-kde4-zh-to-en\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/charliealex123/marian-finetuned-kde4-zh-to-en into local empty directory.\n",
      "Download file model.safetensors:   0%|          | 17.5k/296M [00:00<?, ?B/s]\n",
      "Download file model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 294M/296M [02:55<00:00, 1.45MB/s] \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Download file model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296M/296M [03:10<00:00, 1.45MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Download file model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296M/296M [06:03<00:00, 852kB/s] \n",
      "Download file training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.74k/4.74k [06:03<?, ?B/s]\n",
      "\n",
      "\u001b[A\n",
      "Clean file training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.74k/4.74k [06:03<00:00, 10.5B/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Clean file model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296M/296M [03:06<00:00, 1.66MB/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/Users/alexlo/Desktop/Project/Chai_Trans/marian-finetuned-kde4-zh-to-en-local\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from getpass import getpass\n",
    "\n",
    "# å¡«å¯«ç”¨æˆ¶åå’Œå¯†ç¢¼\n",
    "# username = input(\"Enter your username: \")\n",
    "# password = getpass(\"Enter your password: \")\n",
    "model_path = \"/Users/alexlo/Desktop/Project/Chai_Trans/marian-finetuned-kde4-zh-to-en\"  # æ¨¡å‹æ–‡ä»¶çš„è·¯å¾‘\n",
    "\n",
    "# èªè­‰ä¸¦å‰µå»ºAPI\n",
    "api = HfApi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.52s/it]t]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, BLEU score: 40.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.15s/it]t]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, BLEU score: 49.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.32s/it]t]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, BLEU score: 48.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        # api.upload_folder(token=\"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\",\n",
    "        #         folder_path='/Users/alexlo/Desktop/Project/Chai_Trans/marian-finetuned-kde4-zh-to-en',\n",
    "        #         repo_id=repo_name,\n",
    "        #         commit_message=f\"Training in progress epoch {epoch}\", \n",
    "        #         )\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9:55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.upload_folder(token=\"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\",\n",
    "        folder_path='/Users/alexlo/Desktop/Project/Chai_Trans/marian-finetuned-kde4-zh-to-en',\n",
    "        repo_id=repo_name,\n",
    "        commit_message=f\"Training in progress epoch {epoch}\", \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.push_to_hub(\n",
    "    commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HfApi.upload_file() missing 2 required keyword-only arguments: 'path_or_fileobj' and 'path_in_repo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexlo/Desktop/Project/Chai_Trans/test.ipynb å„²å­˜æ ¼ 46\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexlo/Desktop/Project/Chai_Trans/test.ipynb#Y150sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m api\u001b[39m.\u001b[39;49mupload_file(token\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhf_CncNOiSoVwkdpqWjljyCkUBMTUDkoNCwBP\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexlo/Desktop/Project/Chai_Trans/test.ipynb#Y150sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         repo_id\u001b[39m=\u001b[39;49mrepo_name,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexlo/Desktop/Project/Chai_Trans/test.ipynb#Y150sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         commit_message\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTraining in progress epoch \u001b[39;49m\u001b[39m{\u001b[39;49;00mepoch\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexlo/Desktop/Project/Chai_Trans/test.ipynb#Y150sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/hf_api.py:1208\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_as_future(fn, \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1207\u001b[0m \u001b[39m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1208\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: HfApi.upload_file() missing 2 required keyword-only arguments: 'path_or_fileobj' and 'path_in_repo'"
     ]
    }
   ],
   "source": [
    "api.upload_file(token=\"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\",\n",
    "        repo_id=repo_name,\n",
    "        commit_message=f\"Training in progress epoch {epoch}\", \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(workdata_path)\n",
    "raw_datasets = load_dataset('json', data_files='tmx_zh_en.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310M/310M [04:16<00:00, 1.21MB/s] \n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 416/416 [00:00<00:00, 537kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"charliealex123/marian-finetuned-kde4-zh-to-en\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48894"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …1æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œå…¶ä¸­è©²è‡³å°‘ä¸€å‡¹æ§½åŒ…æ‹¬å…©å‡¹æ§½ï¼Œè©²äº›æ”åƒå…ƒä»¶åˆ†åˆ¥è¨­ç½®æ–¼è©²å…©å‡¹æ§½å…§ã€‚\n",
      "åŸç¿»è­¯: The circuit board module according to claim 1, wherein the at least one groove has two grooves, and the two camera elements are respectively disposed in the two grooves.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: A circuit board module, as described in request 1, in which at least one groove includes two grooves, each of which is set separately in the two grooves.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: åœ¨æœ¬ç™¼æ˜çš„ä¸€å¯¦æ–½ä¾‹ä¸­ï¼Œä¸Šè¿°çš„é›»è·¯æ¿æœ¬é«”åœ¨å…©å‡¹æ§½ä¹‹é–“åŒ…æ‹¬ä¸€ç„¡å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µï¼Œå…©æ”åƒå…ƒä»¶é€éè©²æ¥é›»è·¯æ¿å½¼æ­¤é›»æ€§é€£æ¥ï¼Œè€Œç„¡æ³•é€éè©²å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µé›»æ€§é€£æ¥ã€‚\n",
      "åŸç¿»è­¯: In an embodiment of the disclosure, the circuit board body includes a conducting wiring section without two elements between the two grooves. The two camera elements are electrically connected to each other through the circuit board, but not electronically connecting to each other through the conducting wiring section without two elements.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: In an illustration of the disclosure, the above-mentioned circuit board body includes a no-two-component steering line segment between the two grooves, the two camera elements are connected electrically to each other through the circuit board, and cannot be connected electrically through the two parts leading to the line segment.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …2æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œå…¶ä¸­è©²é›»è·¯æ¿æœ¬é«”åœ¨è©²å…©å‡¹æ§½ä¹‹é–“åŒ…æ‹¬ä¸€ç„¡å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µï¼Œè©²å…©æ”åƒå…ƒä»¶é€éè©²å¤–æ¥é›»è·¯æ¿å½¼æ­¤é›»æ€§é€£æ¥ï¼Œè€Œç„¡æ³•é€éè©²ç„¡å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µé›»æ€§é€£æ¥ã€‚\n",
      "åŸç¿»è­¯: The circuit board module according to claim 2, wherein the circuit board body has a conducting wiring section without two elements between the two grooves, and the two camera elements are electrically connected to each other through the circuit board, but not electronically connecting to each other through the conducting wiring section without two elements.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: The circuit board module, as described in request 2, in which the board body includes a line segment between the two grooves, the two camera elements are connected electrically through the external circuit board, and cannot be connected electrically by the line segment through the non-dividual link.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: ä¸€å¤–æ¥é›»è·¯æ¿ï¼ŒåŒ…æ‹¬å…©é€£æ¥åŸ ï¼Œå…¶ä¸­è©²å…©é€£æ¥åŸ å¯æ’æ‹”åœ°é€£æ¥æ–¼è©²å…©æ¿å°æ¿é€£æ¥å™¨ï¼Œä»¥ä½¿è©²å¤–æ¥é›»è·¯æ¿é›»æ€§é€£æ¥æ–¼è©²é›»è·¯æ¿æœ¬é«”ã€‚\n",
      "åŸç¿»è­¯: an external circuit board having two connection ports pluggablely connected to the two board-to-board connectors, so that the external circuit board is electrically connected to the circuit board body.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: The outer circuit board, which includes two ports, can be connected interpolated to the two pairs so that the outer circuit board is connected electrically to the board body.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¤–æ¥é›»è·¯æ¿åŒ…æ‹¬å…©é€£æ¥åŸ ï¼Œå…¶ä¸­å…©é€£æ¥åŸ å¯æ’æ‹”åœ°é€£æ¥æ–¼å…©æ¿å°æ¿é€£æ¥å™¨ï¼Œä»¥ä½¿å¤–æ¥é›»è·¯æ¿é›»æ€§é€£æ¥æ–¼é›»è·¯æ¿æœ¬é«”ã€‚\n",
      "åŸç¿»è­¯: The external circuit board includes two connection ports pluggablely connected to the two board-to-board connectors, so that the external circuit board is electrically connected to the circuit board body.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: The external circuit board consists of two ports, two of which can be connected interpolated to the two pairs of panels so that the external circuit board is connected electrically to the board body.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´å‰0~10\n",
    "for i in range(1000, 1005):\n",
    "    # i = 3\n",
    "    zh_sentence = raw_datasets['train']['zh'][i]\n",
    "    print('éœ€ç¿»è­¯å¥:', zh_sentence)\n",
    "    print('åŸç¿»è­¯:', raw_datasets['train']['en'][i])\n",
    "    print('æ©Ÿå™¨å­¸ç¿’ç¿»è­¯:', translator(zh_sentence)[0]['translation_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "éœ€ç¿»è­¯å¥: ç„¦è·èª¿ç¯€æ©Ÿæ§‹\n",
      "åŸç¿»è­¯: FOCAL LENGTH ADJUSTMENT MECHANISM\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: Focal length adjustment mechanism\n",
      "\n",
      "éœ€ç¿»è­¯å¥: ä¸€ç¨®ç„¦è·èª¿ç¯€æ©Ÿæ§‹ï¼Œé©æ–¼èª¿ç¯€ä½æ–¼ä¸€æŠ•å½±æ©Ÿçš„ä¸€æ®¼é«”å…§çš„ä¸€ç„¦è·èª¿ç¯€ä»¶ã€‚\n",
      "åŸç¿»è­¯: A focal length adjustment mechanism, adapted for adjusting a focal length adjustment device located in a housing of a projector.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: A focal length adjustment mechanism, suitable for adjusting a focal length adjustment device located in a shell of a projector.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: ç„¦è·èª¿ç¯€æ©Ÿæ§‹åŒ…æ‹¬ä¸€æ—‹éˆ•åŠä¸€ç¬¬ä¸€è¡Œç¨‹èª¿æ•´ä»¶ã€‚\n",
      "åŸç¿»è­¯: The focal length adjustment mechanism includes a knob and a first stroke adjustment member.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: The focal length adjustment mechanism includes a knob and a first process adjustment member.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: æ—‹éˆ•å±€éƒ¨åœ°å¤–éœ²æ–¼æ®¼é«”ã€‚\n",
      "åŸç¿»è­¯: The knob is partially exposed to the housing.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: The knob is partially exposed to the shell.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: æ—‹éˆ•è½‰å‹•ï¼Œå¸¶å‹•ç¬¬ä¸€è¡Œç¨‹èª¿æ•´ä»¶ä»¥ç¬¬ä¸€æ¨è»¸ç‚ºä¸­å¿ƒè½‰å‹•ã€‚\n",
      "åŸç¿»è­¯: The knob rotates to drive the first stroke adjustment member to rotate along the first pivot.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: The knob rotates, with the first movement adjustment member rotates along the first pivot.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: Description of Related Art\n",
      "åŸç¿»è­¯: Description of Related Art\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: Description of Retained Art\n",
      "\n",
      "éœ€ç¿»è­¯å¥: ä¸€èˆ¬ä¾†èªªï¼ŒæŠ•å½±æ©Ÿä½¿ç”¨æ™‚éœ€è¦èª¿æ•´é¡é ­ç„¦è·ï¼Œä»¥ä½¿æŠ•å½±ç•«é¢æ¸…æ¥šã€‚\n",
      "åŸç¿»è­¯: Generally, when using the projector, the focal length of the lens needs to be adjusted to make the projected image clear.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: Generally, the projector is used to adjust the focal length of the mirror so that the projector is clear.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: éš¨è‘—æŠ•å½±æ©Ÿçš„å°ºå¯¸è¶Šä¾†è¶Šå°ï¼Œå—é™æ–¼å…§éƒ¨ç©ºé–“çš„å°ºå¯¸ï¼Œå…‰æ©Ÿæœå‘å°å‹åŒ–ç™¼å±•ã€‚\n",
      "åŸç¿»è­¯: As the size of a projector becomes smaller, the size of the internal space is limited, and light engines are developing towards miniaturization.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: As the size of the projector becomes smaller and limited to the size of the internal space, the light machine moves towards miniaturization.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: éš¨è‘—æŠ•å½±æ©Ÿçš„å°ºå¯¸è¶Šä¾†è¶Šå°ï¼Œå—é™æ–¼å…§éƒ¨ç©ºé–“çš„å°ºå¯¸ï¼Œå…‰æ©Ÿæœå‘å°å‹åŒ–ç™¼å±•ã€‚é€™ä½¿å¾—å…‰æ©Ÿçš„é¡é ­è¼ƒé›£å°ç„¦ã€‚\n",
      "åŸç¿»è­¯: As the size of a projector becomes smaller, the size of the internal space is limited, and light engines are developing towards miniaturization, which makes the lens of the light engines more difficult to focus.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: As the size of the projector becomes smaller, limited to the size of the internal space, the light machine moves towards miniaturization. This makes the mirror of the light machine harder to focus.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: WHAT IS CLAIMED IS:\n",
      "åŸç¿»è­¯: WHAT IS CLAIMED IS:\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: WHAT IS CLAIMED IS:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# æ‹¿100å¥è©±è¨“ç·´å¾Œ\n",
    "for i in range(0, 10):\n",
    "    # i = 3\n",
    "    zh_sentence = raw_datasets['train']['zh'][i]\n",
    "    print('éœ€ç¿»è­¯å¥:', zh_sentence)\n",
    "    print('åŸç¿»è­¯:', raw_datasets['train']['en'][i])\n",
    "    print('æ©Ÿå™¨å­¸ç¿’ç¿»è­¯:', translator(zh_sentence)[0]['translation_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …1æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œå…¶ä¸­è©²è‡³å°‘ä¸€å‡¹æ§½åŒ…æ‹¬å…©å‡¹æ§½ï¼Œè©²äº›æ”åƒå…ƒä»¶åˆ†åˆ¥è¨­ç½®æ–¼è©²å…©å‡¹æ§½å…§ã€‚\n",
      "åŸç¿»è­¯: The circuit board module according to claim 1, wherein the at least one groove has two grooves, and the two camera elements are respectively disposed in the two grooves.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: If requested, the circuit board module described in item 1 should include at least two diagonals in at least one groove in which the camera element is separated into two diagonal slots.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: åœ¨æœ¬ç™¼æ˜çš„ä¸€å¯¦æ–½ä¾‹ä¸­ï¼Œä¸Šè¿°çš„é›»è·¯æ¿æœ¬é«”åœ¨å…©å‡¹æ§½ä¹‹é–“åŒ…æ‹¬ä¸€ç„¡å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µï¼Œå…©æ”åƒå…ƒä»¶é€éè©²æ¥é›»è·¯æ¿å½¼æ­¤é›»æ€§é€£æ¥ï¼Œè€Œç„¡æ³•é€éè©²å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µé›»æ€§é€£æ¥ã€‚\n",
      "åŸç¿»è­¯: In an embodiment of the disclosure, the circuit board body includes a conducting wiring section without two elements between the two grooves. The two camera elements are electrically connected to each other through the circuit board, but not electronically connecting to each other through the conducting wiring section without two elements.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: In the case of this invention, the above-mentioned circuit board body consists of a no-two-part line link between the two dents, which is connected electrically to each other through the circuit board and cannot be connected electrically through the two-part link.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …2æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œå…¶ä¸­è©²é›»è·¯æ¿æœ¬é«”åœ¨è©²å…©å‡¹æ§½ä¹‹é–“åŒ…æ‹¬ä¸€ç„¡å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µï¼Œè©²å…©æ”åƒå…ƒä»¶é€éè©²å¤–æ¥é›»è·¯æ¿å½¼æ­¤é›»æ€§é€£æ¥ï¼Œè€Œç„¡æ³•é€éè©²ç„¡å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µé›»æ€§é€£æ¥ã€‚\n",
      "åŸç¿»è­¯: The circuit board module according to claim 2, wherein the circuit board body has a conducting wiring section without two elements between the two grooves, and the two camera elements are electrically connected to each other through the circuit board, but not electronically connecting to each other through the conducting wiring section without two elements.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: If requested, the circuit board module, which includes a no-two-part line link between the two grooves, is electrically connected through the outside circuit board and cannot be electrically connected through the no-two link.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: ä¸€å¤–æ¥é›»è·¯æ¿ï¼ŒåŒ…æ‹¬å…©é€£æ¥åŸ ï¼Œå…¶ä¸­è©²å…©é€£æ¥åŸ å¯æ’æ‹”åœ°é€£æ¥æ–¼è©²å…©æ¿å°æ¿é€£æ¥å™¨ï¼Œä»¥ä½¿è©²å¤–æ¥é›»è·¯æ¿é›»æ€§é€£æ¥æ–¼è©²é›»è·¯æ¿æœ¬é«”ã€‚\n",
      "åŸç¿»è­¯: an external circuit board having two connection ports pluggablely connected to the two board-to-board connectors, so that the external circuit board is electrically connected to the circuit board body.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: The outer circuit board consists of two ports, of which the two docks can be connected interpolated to the two pairs so that the outer circuit board will be connected electrically to the board itself.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¤–æ¥é›»è·¯æ¿åŒ…æ‹¬å…©é€£æ¥åŸ ï¼Œå…¶ä¸­å…©é€£æ¥åŸ å¯æ’æ‹”åœ°é€£æ¥æ–¼å…©æ¿å°æ¿é€£æ¥å™¨ï¼Œä»¥ä½¿å¤–æ¥é›»è·¯æ¿é›»æ€§é€£æ¥æ–¼é›»è·¯æ¿æœ¬é«”ã€‚\n",
      "åŸç¿»è­¯: The external circuit board includes two connection ports pluggablely connected to the two board-to-board connectors, so that the external circuit board is electrically connected to the circuit board body.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: The outer circuit board consists of two ports, two of which can be connected interpolated to two pairs to make the outer circuit board electrically connected to the original circuit board.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …1æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œå…¶ä¸­è©²å¤–æ¥é›»è·¯æ¿åŒ…æ‹¬ä¸€ä¸»å€æ®µåŠå¾è©²ä¸»å€æ®µå»¶ä¼¸å‡ºçš„å…©åˆ†æ”¯ï¼Œè©²å…©é€£æ¥åŸ åˆ†åˆ¥è¨­ç½®æ–¼è©²å…©åˆ†æ”¯ä¸”å¯æ’æ‹”åœ°é€£æ¥æ–¼è©²å…©æ¿å°æ¿é€£æ¥å™¨ã€‚\n",
      "åŸç¿»è­¯: The circuit board module according to claim 1, wherein the external circuit board has a main section and two branches extending from the main section, and the two connection ports are respectively disposed on the two branches and pluggably connected to the two board-to-board connectors.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: If requested, the circuit board module, which includes a main section and two branches extending from the main section, shall be separated between the two branches and may be plugged into the two pairs.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …1æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œæ›´åŒ…æ‹¬ï¼š\n",
      "åŸç¿»è­¯: The circuit board module according to claim 1, further comprising:\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: If requested, the circuit board module described in item 1, more specifically:\n",
      "\n",
      "éœ€ç¿»è­¯å¥: åœ¨æœ¬ç™¼æ˜çš„ä¸€å¯¦æ–½ä¾‹ä¸­ï¼Œä¸Šè¿°çš„é›»è·¯æ¿æ¨¡çµ„æ›´åŒ…æ‹¬è‡³å°‘ä¸€ç¬¬äºŒé›»å­å…ƒä»¶ï¼Œè¨­ç½®æ–¼è¡¨é¢ä¸”é›»æ€§é€£æ¥æ–¼é›»è·¯æ¿æœ¬é«”ï¼Œå„ç¬¬äºŒé›»å­å…ƒä»¶çš„é«˜åº¦å°æ–¼å„æ”åƒå…ƒä»¶çš„é«˜åº¦ã€‚\n",
      "åŸç¿»è­¯: In an embodiment of the disclosure, the circuit board module further includes at least one second electronic element disposed on the surface and electrically connected to the circuit board body. The height of each second electronic element is smaller than the height of each camera element.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: In the case of this invention, the above-mentioned circuit panel module consists of at least one second electrical element, which is designed on the surface and is electrically connected to the original circuit board, with each second electrical element having a height of less than the height of each camera element.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: è‡³å°‘ä¸€ç¬¬äºŒé›»å­å…ƒä»¶ï¼Œè¨­ç½®æ–¼è©²è¡¨é¢ä¸”é›»æ€§é€£æ¥æ–¼è©²é›»è·¯æ¿æœ¬é«”ï¼Œå„è©²ç¬¬äºŒé›»å­å…ƒä»¶çš„é«˜åº¦å°æ–¼å„è©²æ”åƒå…ƒä»¶çš„é«˜åº¦ã€‚\n",
      "åŸç¿»è­¯: at least one second electronic element disposed on the surface and electrically connected to the circuit board body, the height of each second electronic element is smaller than the height of each camera element.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: At least one second electrical element, which is placed on that surface and is electrically connected to the original circuit board, shall each be of a height less than the height of the respective camera element.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …6æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œå…¶ä¸­å„è©²ç¬¬ä¸€é›»å­å…ƒä»¶åŒ…æ‹¬ä¸€éº¥å…‹é¢¨ï¼Œå„è©²ç¬¬äºŒé›»å­å…ƒä»¶åŒ…æ‹¬ä¸€å…‰æºã€‚\n",
      "åŸç¿»è­¯: The circuit board module according to claim 6, wherein each of the first electronic elements has a microphone, and each of the second electronic elements has a light source.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: If requested, the circuit board module described in item 6, each of which includes a microphone and each of which includes a light source.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´å‰1000~1010\n",
    "for i in range(1000, 1010):\n",
    "    # i = 3\n",
    "    zh_sentence = raw_datasets['train']['zh'][i]\n",
    "    print('éœ€ç¿»è­¯å¥:', zh_sentence)\n",
    "    print('åŸç¿»è­¯:', raw_datasets['train']['en'][i])\n",
    "    print('æ©Ÿå™¨å­¸ç¿’ç¿»è­¯:', translator(zh_sentence)[0]['translation_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …1æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œå…¶ä¸­è©²è‡³å°‘ä¸€å‡¹æ§½åŒ…æ‹¬å…©å‡¹æ§½ï¼Œè©²äº›æ”åƒå…ƒä»¶åˆ†åˆ¥è¨­ç½®æ–¼è©²å…©å‡¹æ§½å…§ã€‚\n",
      "åŸç¿»è­¯: The circuit board module according to claim 1, wherein the at least one groove has two grooves, and the two camera elements are respectively disposed in the two grooves.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: If requested, the circuit board module described in item 1 should include at least two diagonals in at least one groove in which the camera element is separated into two diagonal slots.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: åœ¨æœ¬ç™¼æ˜çš„ä¸€å¯¦æ–½ä¾‹ä¸­ï¼Œä¸Šè¿°çš„é›»è·¯æ¿æœ¬é«”åœ¨å…©å‡¹æ§½ä¹‹é–“åŒ…æ‹¬ä¸€ç„¡å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µï¼Œå…©æ”åƒå…ƒä»¶é€éè©²æ¥é›»è·¯æ¿å½¼æ­¤é›»æ€§é€£æ¥ï¼Œè€Œç„¡æ³•é€éè©²å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µé›»æ€§é€£æ¥ã€‚\n",
      "åŸç¿»è­¯: In an embodiment of the disclosure, the circuit board body includes a conducting wiring section without two elements between the two grooves. The two camera elements are electrically connected to each other through the circuit board, but not electronically connecting to each other through the conducting wiring section without two elements.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: In the case of this invention, the above-mentioned circuit board body consists of a no-two-part line link between the two dents, which is connected electrically to each other through the circuit board and cannot be connected electrically through the two-part link.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …2æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œå…¶ä¸­è©²é›»è·¯æ¿æœ¬é«”åœ¨è©²å…©å‡¹æ§½ä¹‹é–“åŒ…æ‹¬ä¸€ç„¡å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µï¼Œè©²å…©æ”åƒå…ƒä»¶é€éè©²å¤–æ¥é›»è·¯æ¿å½¼æ­¤é›»æ€§é€£æ¥ï¼Œè€Œç„¡æ³•é€éè©²ç„¡å…©å…ƒä»¶å°é€šèµ°ç·šå€æ®µé›»æ€§é€£æ¥ã€‚\n",
      "åŸç¿»è­¯: The circuit board module according to claim 2, wherein the circuit board body has a conducting wiring section without two elements between the two grooves, and the two camera elements are electrically connected to each other through the circuit board, but not electronically connecting to each other through the conducting wiring section without two elements.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: If requested, the circuit board module, which includes a no-two-part line link between the two grooves, is electrically connected through the outside circuit board and cannot be electrically connected through the no-two link.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: ä¸€å¤–æ¥é›»è·¯æ¿ï¼ŒåŒ…æ‹¬å…©é€£æ¥åŸ ï¼Œå…¶ä¸­è©²å…©é€£æ¥åŸ å¯æ’æ‹”åœ°é€£æ¥æ–¼è©²å…©æ¿å°æ¿é€£æ¥å™¨ï¼Œä»¥ä½¿è©²å¤–æ¥é›»è·¯æ¿é›»æ€§é€£æ¥æ–¼è©²é›»è·¯æ¿æœ¬é«”ã€‚\n",
      "åŸç¿»è­¯: an external circuit board having two connection ports pluggablely connected to the two board-to-board connectors, so that the external circuit board is electrically connected to the circuit board body.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: The outer circuit board consists of two ports, of which the two docks can be connected interpolated to the two pairs so that the outer circuit board will be connected electrically to the board itself.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¤–æ¥é›»è·¯æ¿åŒ…æ‹¬å…©é€£æ¥åŸ ï¼Œå…¶ä¸­å…©é€£æ¥åŸ å¯æ’æ‹”åœ°é€£æ¥æ–¼å…©æ¿å°æ¿é€£æ¥å™¨ï¼Œä»¥ä½¿å¤–æ¥é›»è·¯æ¿é›»æ€§é€£æ¥æ–¼é›»è·¯æ¿æœ¬é«”ã€‚\n",
      "åŸç¿»è­¯: The external circuit board includes two connection ports pluggablely connected to the two board-to-board connectors, so that the external circuit board is electrically connected to the circuit board body.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: The outer circuit board consists of two ports, two of which can be connected interpolated to two pairs to make the outer circuit board electrically connected to the original circuit board.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …1æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œå…¶ä¸­è©²å¤–æ¥é›»è·¯æ¿åŒ…æ‹¬ä¸€ä¸»å€æ®µåŠå¾è©²ä¸»å€æ®µå»¶ä¼¸å‡ºçš„å…©åˆ†æ”¯ï¼Œè©²å…©é€£æ¥åŸ åˆ†åˆ¥è¨­ç½®æ–¼è©²å…©åˆ†æ”¯ä¸”å¯æ’æ‹”åœ°é€£æ¥æ–¼è©²å…©æ¿å°æ¿é€£æ¥å™¨ã€‚\n",
      "åŸç¿»è­¯: The circuit board module according to claim 1, wherein the external circuit board has a main section and two branches extending from the main section, and the two connection ports are respectively disposed on the two branches and pluggably connected to the two board-to-board connectors.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: If requested, the circuit board module, which includes a main section and two branches extending from the main section, shall be separated between the two branches and may be plugged into the two pairs.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …1æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œæ›´åŒ…æ‹¬ï¼š\n",
      "åŸç¿»è­¯: The circuit board module according to claim 1, further comprising:\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: If requested, the circuit board module described in item 1, more specifically:\n",
      "\n",
      "éœ€ç¿»è­¯å¥: åœ¨æœ¬ç™¼æ˜çš„ä¸€å¯¦æ–½ä¾‹ä¸­ï¼Œä¸Šè¿°çš„é›»è·¯æ¿æ¨¡çµ„æ›´åŒ…æ‹¬è‡³å°‘ä¸€ç¬¬äºŒé›»å­å…ƒä»¶ï¼Œè¨­ç½®æ–¼è¡¨é¢ä¸”é›»æ€§é€£æ¥æ–¼é›»è·¯æ¿æœ¬é«”ï¼Œå„ç¬¬äºŒé›»å­å…ƒä»¶çš„é«˜åº¦å°æ–¼å„æ”åƒå…ƒä»¶çš„é«˜åº¦ã€‚\n",
      "åŸç¿»è­¯: In an embodiment of the disclosure, the circuit board module further includes at least one second electronic element disposed on the surface and electrically connected to the circuit board body. The height of each second electronic element is smaller than the height of each camera element.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: In the case of this invention, the above-mentioned circuit panel module consists of at least one second electrical element, which is designed on the surface and is electrically connected to the original circuit board, with each second electrical element having a height of less than the height of each camera element.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: è‡³å°‘ä¸€ç¬¬äºŒé›»å­å…ƒä»¶ï¼Œè¨­ç½®æ–¼è©²è¡¨é¢ä¸”é›»æ€§é€£æ¥æ–¼è©²é›»è·¯æ¿æœ¬é«”ï¼Œå„è©²ç¬¬äºŒé›»å­å…ƒä»¶çš„é«˜åº¦å°æ–¼å„è©²æ”åƒå…ƒä»¶çš„é«˜åº¦ã€‚\n",
      "åŸç¿»è­¯: at least one second electronic element disposed on the surface and electrically connected to the circuit board body, the height of each second electronic element is smaller than the height of each camera element.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: At least one second electrical element, which is placed on that surface and is electrically connected to the original circuit board, shall each be of a height less than the height of the respective camera element.\n",
      "\n",
      "éœ€ç¿»è­¯å¥: å¦‚è«‹æ±‚é …6æ‰€è¿°çš„é›»è·¯æ¿æ¨¡çµ„ï¼Œå…¶ä¸­å„è©²ç¬¬ä¸€é›»å­å…ƒä»¶åŒ…æ‹¬ä¸€éº¥å…‹é¢¨ï¼Œå„è©²ç¬¬äºŒé›»å­å…ƒä»¶åŒ…æ‹¬ä¸€å…‰æºã€‚\n",
      "åŸç¿»è­¯: The circuit board module according to claim 6, wherein each of the first electronic elements has a microphone, and each of the second electronic elements has a light source.\n",
      "æ©Ÿå™¨å­¸ç¿’ç¿»è­¯: If requested, the circuit board module described in item 6, each of which includes a microphone and each of which includes a light source.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´å¾Œ1000~1010\n",
    "for i in range(1000, 1010):\n",
    "    # i = 3\n",
    "    zh_sentence = raw_datasets['train']['zh'][i]\n",
    "    print('éœ€ç¿»è­¯å¥:', zh_sentence)\n",
    "    print('åŸç¿»è­¯:', raw_datasets['train']['en'][i])\n",
    "    print('æ©Ÿå™¨å­¸ç¿’ç¿»è­¯:', translator(zh_sentence)[0]['translation_text'])\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
